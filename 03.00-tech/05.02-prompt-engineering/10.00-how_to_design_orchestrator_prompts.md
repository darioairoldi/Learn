---
title: "How to Design Orchestrator Prompts for Multi-Agent Workflows"
author: "Dario Airoldi"
date: "2026-02-22"
categories: [tech, prompt-engineering, github-copilot]
description: "Learn how to design orchestrator prompts that coordinate multiple specialized agents through structured phase-based workflows, delegation patterns, and architecture decisions in GitHub Copilot."
---

# How to design orchestrator prompts for multi-agent workflows

> When a single prompt can't handle a complex, multi-step task, you need an **orchestrator**â€”a coordinator agent that delegates subtasks to specialized agents and synthesizes their results. This article teaches you how to design orchestrator prompts that are clear, maintainable, and effective.

Up to this point in the series, you've learned how to structure individual customization files: [instructions](./05.00-how_to_structure_content_for_copilot_instruction_files.md), [prompts](./03.00-how_to_structure_content_for_copilot_prompt_files.md), [agents](./04.00-how_to_structure_content_for_copilot_agent_files.md), [skills](./06.00-how_to_structure_content_for_copilot_skills.md), [hooks](./09.00-how_to_use_agent_hooks_for_lifecycle_automation.md), and [tools](./09.50-how_to_leverage_tools_in_prompt_orchestrations.md). Each one handles a specific piece of the customization puzzle. But real-world workflows often require **multiple agents working together**â€”a planner gathering context, a builder implementing changes, a reviewer checking quality.

This article bridges that gap. It introduces the orchestrator patternâ€”the architectural foundation for multi-agent coordinationâ€”and teaches you how to design orchestrator prompts that decompose complex tasks into clear phases, delegate work to specialized agents, and control execution flow. Later articles cover the mechanics in detail: [subagent APIs](./11.00-how_to_design_subagent_orchestrations.md), [information flow](./12.00-how_to_manage_information_flow_during_prompt_orchestrations.md), and [token optimization](./13.00-how_to_optimize_token_consumption_during_prompt_orchestrations.md).

## Table of contents

- [ğŸ¯ The problem: monolithic prompts don't scale](#-the-problem-monolithic-prompts-dont-scale)
- [ğŸ—ï¸ The orchestrator pattern](#ï¸-the-orchestrator-pattern)
- [ğŸ“‹ Designing specialized agents](#-designing-specialized-agents)
- [ğŸ”€ Phase-based coordination](#-phase-based-coordination)
- [ğŸ¯ The use case challenge methodology](#-the-use-case-challenge-methodology)
- [ğŸ§  Architecture decisions: when to orchestrate](#-architecture-decisions-when-to-orchestrate)
- [âš™ï¸ Execution flow control](#ï¸-execution-flow-control)
- [ğŸ›ï¸ Context engineering for orchestrators](#ï¸-context-engineering-for-orchestrators)
- [ğŸ’¡ Key design principles](#-key-design-principles)
- [ğŸ¯ Conclusion](#-conclusion-1)
- [ğŸ“š References](#-references)

---

# ğŸ¯ The problem: monolithic prompts don't scale

Consider a task like "restructure our authentication module to use OAuth 2.0." A single monolithic prompt would need to:

1. Research the existing auth implementation
2. Analyze OAuth 2.0 requirements
3. Design the migration architecture
4. Implement the changes across multiple files
5. Update tests and documentation
6. Review the changes for security issues

That's six fundamentally different activities crammed into one context window. The model loses focus, forgets constraints from step 1 by the time it reaches step 5, and you can't intervene between phases to steer direction. The result is often a sprawling, uncontrollable session that drifts from the original intent.

The core issues with monolithic prompts:

| Problem | Impact |
|---------|--------|
| **<mark>Context window bloat</mark>** | Earlier instructions get pushed out as the session grows |
| **<mark>Role confusion</mark>** | The model switches between researcher, architect, and coder without clear boundaries |
| **<mark>No checkpoints</mark>** | You can't review intermediate results before the model moves on |
| **<mark>Wasted tokens</mark>** | Research context consumed during implementation has no further value |
| **<mark>Debugging difficulty</mark>** | When something goes wrong, you can't isolate which phase caused it |

The orchestrator pattern solves these problems by decomposing complex tasks into phases, each handled by a focused agent with its own context window.

---

# ğŸ—ï¸ The orchestrator pattern

## Core architecture

An orchestrator is a **coordinator agent** that doesn't do the detailed work itself. Instead, it:

1. **Decomposes** the task into discrete phases
2. **Delegates** each phase to a specialized agent
3. **Synthesizes** the results into a coherent outcome
4. **Controls** the execution flow between phases

Think of it as a project manager. The PM doesn't write code, design databases, or run testsâ€”they coordinate the people who do, ensuring the right work happens in the right order with the right information.

### The coordinator and worker model

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              ORCHESTRATOR                   â”‚
â”‚                                             â”‚
â”‚  "For each feature request:"                â”‚
â”‚  1. Delegate research to â†’ Researcher       â”‚
â”‚  2. Review findings, decide approach        â”‚
â”‚  3. Delegate implementation to â†’ Builder    â”‚
â”‚  4. Delegate review to â†’ Reviewer           â”‚
â”‚  5. If issues found â†’ Builder (fix cycle)   â”‚
â”‚  6. Synthesize final result                 â”‚
â”‚                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚              â”‚              â”‚
        â–¼              â–¼              â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Researcherâ”‚  â”‚  Builder  â”‚  â”‚ Reviewer  â”‚
  â”‚           â”‚  â”‚           â”‚  â”‚           â”‚
  â”‚ read-only â”‚  â”‚ edit, run â”‚  â”‚ read-only â”‚
  â”‚  tools    â”‚  â”‚   tools   â”‚  â”‚  tools    â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

Each specialist operates in its own **isolated context window**. The orchestrator receives only the condensed resultsâ€”not the full research trail or implementation attempt. This isolation is the key architectural advantage: it keeps the coordinator's context focused on *decisions*, not details.

## Execution contexts

Orchestrator workflows can run across three execution environments. Understanding these helps you design orchestrators that match your task's requirements:

| Context | Where it runs | Interaction | Best for |
|---------|---------------|-------------|----------|
| **<mark>Local agent</mark>** | Your VS Code instance | Interactive, real-time | Research, planning, iterative design |
| **<mark>Background agent</mark>** | CLI-based, local machine | Non-interactive | Well-defined implementation tasks |
| **<mark>Cloud agent</mark>** | Remote infrastructure | Async, via PRs | Team collaboration, autonomous tasks |

A practical workflow might start local (interactive planning), hand off to background (autonomous implementation), then move to cloud (PR review with the team). VS Code supports **handing off sessions** between these contexts, carrying conversation history forward.

## The Agent HQ pattern

For complex orchestrations, you'll want a central coordination pointâ€”what we call <mark>Agent HQ</mark> (Headquarters). This is the orchestrator agent that serves as the entry point for all multi-step tasks. Agent HQ:

- **Receives** the user's high-level request
- **Plans** the execution phases
- **Selects** the appropriate specialist for each phase
- **Monitors** intermediate results for quality and relevance
- **Decides** whether to proceed, iterate, or abort

You implement Agent HQ as a custom agent file (`.agent.md`) with:
- Read-only tools for research and analysis
- The `agent` tool for spawning subagents
- Clear phase definitions in its instructions
- `agents` frontmatter to restrict which specialists it can invoke

```yaml
---
name: Agent HQ
description: Multi-phase workflow coordinator
tools: ['agent', 'read', 'search', 'fetch']
agents: ['Researcher', 'Builder', 'Reviewer', 'Validator']
---
```

---

# ğŸ“‹ Designing specialized agents

## Agent anatomy

Each specialist agent needs three things defined:

1. **<mark>Role</mark>** â€” What it focuses on (research, implementation, review)
2. **<mark>Tools</mark>** â€” What capabilities it has (and crucially, what it *can't* do)
3. **<mark>Constraints</mark>** â€” Boundaries on its behavior

### Tool access as a design lever

Tool restriction is the most powerful lever for controlling specialist behavior. A reviewer agent with read-only tools **can't** accidentally modify files. A builder agent without the `fetch` tool stays focused on implementation rather than wandering into research.

```yaml
# Reviewer: read-only access prevents accidental changes
---
name: Reviewer
user-invokable: false
tools: ['read', 'search', 'grep']
---
Review code changes for quality, security, and adherence to project patterns.
Focus on actionable feedback. Don't suggest purely stylistic changes.
```

```yaml
# Builder: full editing capabilities, no research tools
---
name: Builder
user-invokable: false
model: ['Claude Haiku 4.5 (copilot)', 'Gemini 3 Flash (Preview) (copilot)']
tools: ['edit', 'read', 'search', 'run_in_terminal']
---
Implement changes according to the plan provided.
Follow existing code patterns. Run tests after making changes.
```

Notice that the builder uses a faster, cheaper model. Specialists with narrow focus often perform well with smaller models, reducing token costs without sacrificing quality.

### The `user-invokable` and `disable-model-invocation` properties

Two frontmatter properties control how agents appear and interact:

| Property | Default | Effect |
|----------|---------|--------|
| `user-invokable` | `true` | Controls whether the agent appears in the agents dropdown |
| `disable-model-invocation` | `false` | Prevents the agent from being auto-invoked as a subagent |

For specialist agents in an orchestration, set `user-invokable: false`. This keeps them out of the dropdownâ€”they're only accessible through the orchestrator. The orchestrator itself should have `user-invokable: true` (the default) so users can select it directly.

## The four-specialist pattern

A practical starting architecture for most orchestration workflows uses four specialist roles:

| Role | Purpose | Tools | Model strategy |
|------|---------|-------|----------------|
| **Researcher** | Gather context, analyze existing code | `read`, `search`, `fetch`, `grep` | Standardâ€”needs strong reasoning |
| **Builder** | Implement changes, write code | `edit`, `read`, `search`, `run_in_terminal` | Can use faster/cheaper model |
| **Validator** | Run tests, check constraints | `read`, `run_in_terminal` | Can use faster/cheaper model |
| **Reviewer** | Quality review, security check | `read`, `search`, `grep` | Standardâ€”needs strong judgment |

This isn't a rigid template. Some workflows need only two specialists (plan + implement). Others need more granular specialization (separate security reviewer, documentation writer, migration specialist). Start with the minimum and add specialists only when you observe clear role confusion in a simpler setup.

### When to split vs. combine roles

**Split** when:
- Two roles need *different tool access* (reviewer shouldn't edit, builder shouldn't research all day)
- The work produces *large intermediate artifacts* that would bloat a shared context
- You want *independent quality gates* between phases

**Combine** when:
- The roles need the *same tools and context* to be effective
- The combined work fits comfortably in one context window
- Splitting would create too much handoff overhead

---

# ğŸ”€ Phase-based coordination

## Designing the phase flow

An orchestrator's central design challenge is defining the **phase sequence**â€”which work happens when, what information flows between phases, and where humans can intervene.

### Linear flow

The simplest pattern. Each phase completes before the next begins:

```
Research â†’ Plan â†’ Implement â†’ Test â†’ Review â†’ Done
```

Good for well-understood tasks with clear sequential dependencies. The output of each phase feeds into the next.

### Iterative flow

Phases can loop back when quality gates aren't met:

```
Research â†’ Plan â†’ Implement â†’ Review â”€â”
                     â–²                 â”‚
                     â””â”€â”€ (issues) â”€â”€â”€â”€â”€â”˜
```

The orchestrator decides whether to iterate based on the reviewer's findings. Set iteration limits to prevent infinite loops:

```markdown
Iterate between implementation and review at most 3 times.
If the reviewer still reports critical issues after 3 iterations,
stop and present the current state with remaining issues for human review.
```

### Parallel flow

Independent phases run simultaneously:

```
         â”Œâ†’ Security review â”€â”€â”
Task â†’ â”€â”€â”¤â†’ Performance check â”œâ†’ Synthesize â†’ Done
         â””â†’ Test generation â”€â”€â”˜
```

VS Code can spawn multiple subagents concurrently. Use parallel flow when phases don't depend on each other's outputâ€”for example, running multiple review perspectives at once. The orchestrator waits for all results before continuing.

## Handoff configuration

When using <mark>agent-level handoffs</mark> (not subagents), the `handoffs` frontmatter controls transitions between orchestation phases:

```yaml
---
name: Planner
tools: ['read', 'search']
handoffs:
  - label: Start Implementation
    agent: Builder
    prompt: "Implement the plan outlined above."
    send: false
  - label: Request More Research
    agent: Researcher
    prompt: "Research the following areas identified in the plan..."
    send: false
---
```

The `send` property determines automation level:

| `send` value | Behavior | Best for |
|-------------|----------|----------|
| <mark>`false`</mark> (default) | Shows button, user clicks to proceed | Review checkpoints, human-in-the-loop |
| <mark>`true`</mark> | Automatically starts the next phase | Fully automated pipelines |

**Design guidance:** Default to `send: false` for phases where human review adds value (after planning, after implementation). Use `send: true` only for transitions that don't benefit from human inspection (automatic test runs after implementation).

### Handoffs vs. subagents

Handoffs and subagents serve different coordination purposes:

| Aspect | Handoffs | Subagents |
|--------|----------|-----------|
| **Control** | <mark>User-driven transitions</mark> | <mark>Agent-initiated delegation</mark> |
| **Context** | Full conversation carries forward | <mark>Isolated context</mark>, only summary returns |
| **Visibility** | New session with history | Collapsed tool call in main session |
| **Best for** | <mark>Sequential workflows</mark> with review gates | <mark>Research</mark>, <mark>analysis</mark>, <mark>parallel tasks</mark> |

Use handoffs when you want users to review and approve each phase. Use <mark>subagents</mark> when the orchestrator should autonomously delegate work and synthesize results. Many real workflows combine <mark>bothâ€”handoffs</mark> for major phase boundaries and <mark>subagents</mark> for focused subtasks within phases.

---

# ğŸ¯ The use case challenge methodology

Before building an orchestrator, you need to validate that your use case actually requires one. The <mark>Use Case Challenge</mark> is a structured validation methodology that prevents over-engineering.

## The challenge framework

For each proposed orchestrator, answer these five questions:

### 1. Complexity check

> "Can a single agent with good instructions handle this task?"

<mark>If yes, you don't need an orchestrator</mark>. A well-structured agent file with clear instructions often outperforms a poorly designed multi-agent system. Always try the simple approach first.

**Signs you need orchestration:**
- The task requires fundamentally different tool sets at different stages
- Context window fills up before the task completes
- You need quality gates between phases
- Different phases benefit from different models or personas

### 2. Role definition

> "Can you clearly name and describe each specialist's role in one sentence?"

<mark>If you can't articulate what each agent does in a single sentence, your decomposition is wrong</mark>. Vague roles ("Helper Agent", "Support Agent") indicate unclear boundaries.

**Good roles:**
- "Researcher: Analyze the existing codebase to understand current authentication patterns, dependencies, and test coverage."
- "Builder: Implement the OAuth 2.0 migration according to the approved plan, following existing code patterns."

**Bad roles:**
- "Agent 1: Handle the first part of the work."
- "Support: Help with various tasks as needed."

### 3. Handoff clarity

> "What exactly passes between phasesâ€”and is it clearly structured?"

Every <mark>phase transition needs a defined contract</mark>: what the receiving agent gets and in what format. Vague handoffs ("pass the results") lead to information loss and hallucination.

**Structured handoff:**
```markdown
The Researcher outputs a JSON report:
- `files_analyzed`: list of relevant file paths
- `current_patterns`: description of existing patterns
- `dependencies`: list of affected dependencies
- `risks`: identified migration risks
```

### 4. Failure handling

> "What happens when a specialist fails or produces poor results?"

<mark>Every phase can fail</mark>. Your orchestrator needs explicit strategies for:
- **Retry**: Run the specialist again with adjusted instructions
- **Escalate**: Present the issue to the user for guidance
- **Skip**: Proceed without that phase's output (if non-critical)
- **Abort**: Stop the workflow entirely

### 5. Value verification

> "Does the multi-agent approach measurably improve on the single-agent approach?"

After building v1, compare results. Sometimes the orchestration overhead (design time, token costs, debugging complexity) exceeds the quality improvement. This isn't a failureâ€”it's learning that the task doesn't benefit from decomposition.

---

# ğŸ§  Architecture decisions: when to orchestrate

## The decision framework

Not every complex prompt needs an orchestrator. Use this framework to decide:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Is the task multi-phase?                       â”‚
â”‚                                                         â”‚
â”‚  NO â†’ Use a single agent or prompt file                 â”‚
â”‚  YES â†“                                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚           Do phases need different tools?                â”‚
â”‚                                                         â”‚
â”‚  NO â†’ Consider a single agent with phased instructions  â”‚
â”‚  YES â†“                                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚           Does context fill up within one session?       â”‚
â”‚                                                         â”‚
â”‚  NO â†’ Single agent with clear phase markers works       â”‚
â”‚  YES â†“                                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚           Do you need quality gates between phases?      â”‚
â”‚                                                         â”‚
â”‚  NO â†’ Subagents for context isolation only              â”‚
â”‚  YES â†’ Full orchestrator with handoffs + subagents      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Architecture tiers

| Tier | Setup | When to use |
|------|-------|-------------|
| **Single prompt** | One `.prompt.md` file | Simple, focused tasks |
| **Single agent** | One `.agent.md` with phased instructions | Multi-step but same tool set throughout |
| **Agent + subagents** | One main agent spawning subagents | Context isolation needed, no human checkpoints |
| **Full orchestrator** | Coordinator + specialist agents + handoffs | Complex, multi-phase, different tools per phase |

Start at the simplest tier that handles your requirements. Upgrade when you observe specific limitationsâ€”not proactively.

## Tool composition validation

Before deploying an orchestrator, validate your tool composition:

1. **<mark>Coverage check</mark>** â€” Do your specialists collectively have access to all tools the workflow needs?
2. **<mark>Isolation check</mark>** â€” Do specialists have access to *only* the tools they need? (Principle of least privilege)
3. **<mark>Conflict check</mark>** â€” Might two specialists try to edit the same files simultaneously?
4. **<mark>MCP integration</mark>** â€” Do any specialists need MCP server tools? Are those servers properly configured?

```markdown
## Tool composition audit

| Agent | Tools needed | Tools granted | Gap? | Risk? |
|-------|-------------|---------------|------|-------|
| Researcher | read, search, fetch | read, search, fetch | âœ… None | Low |
| Builder | edit, read, terminal | edit, read, search, terminal | âœ… None | Medium (terminal access) |
| Reviewer | read, search | read, search, grep | âœ… None | Low |
| Validator | read, terminal | read, terminal | âœ… None | Medium (terminal access) |
```

---

# âš™ï¸ Execution flow control

## Iteration control

Orchestrators must set explicit limits on iterative loops to prevent runaway sessions:

```markdown
## Iteration limits

- Implementation â†’ Review cycle: maximum 3 iterations
- Research â†’ Planning refinement: maximum 2 iterations
- Overall workflow: maximum 5 total specialist invocations

If any limit is reached:
1. Summarize the current state
2. List unresolved issues
3. Present to the user for manual decision
```

Without iteration limits, a perfectionist reviewer paired with a compliant builder can loop indefinitelyâ€”each review finding new minor issues, each fix introducing new review surface.

## Recursion prevention

Prevent agents from spawning subagents that spawn more subagents:

```markdown
## Recursion rules

- Specialist agents MUST NOT spawn their own subagents
- Only the orchestrator coordinates specialist invocations
- Maximum orchestration depth: 1 (orchestrator â†’ specialist, never deeper)
```

Use <mark>`disable-model-invocation: true`</mark> on specialist agents and control the `agents` array on the orchestrator to enforce this structurally rather than relying on instructions alone.

## Error handling patterns

| Scenario | Strategy | Implementation |
|----------|----------|----------------|
| Specialist returns empty result | Retry once with explicit diagnostic prompt | "The previous attempt returned no results. Explain what you searched for and what you found." |
| Specialist contradicts plan | Present both perspectives to the user | Include the original plan and the specialist's objection in a summary |
| Tool execution fails | Log the error, try alternative approach | "If `npm test` fails, try `npx jest` directly" |
| Context window exhausted | Summarize current progress, start new session | Create a handoff with a condensed summary of completed work |

## Parallel vs. sequential execution

**<mark>Use parallel execution when:</mark>**
- Tasks are independent (security review AND performance review of the same code)
- You want faster overall completion
- Results don't need to feed into each other

**<mark>Use sequential execution when:</mark>**
- <mark>Later phases depend on earlier results</mark> (implementation depends on research)
- <mark>You need human checkpoints</mark> between phases
- <mark>Order matters for correctness</mark> (tests before review)

```yaml
# Parallel: independent review perspectives
---
name: Thorough Reviewer
tools: ['agent', 'read', 'search']
agents: ['Security Reviewer', 'Performance Reviewer', 'Code Quality Reviewer']
---
Run all three review perspectives as parallel subagents.
Synthesize findings into a single prioritized list.
```

---

# ğŸ›ï¸ Context engineering for orchestrators

## The context budget

Every orchestator operates within a <mark>context window budget</mark>. <mark>The orchestrator's own context should stay lean</mark>â€”it's primarily a decision-making layer, not a processing layer.

### What belongs in the orchestrator's context

| Include | Exclude |
|---------|---------|
| <mark>Phase definitions and sequencing</mark> | Detailed implementation instructions |
| <mark>Specialist selection criteria</mark> | Full code listings |
| <mark>Quality gate thresholds</mark> | Research findings (keep in specialist context) |
| <mark>Iteration limits and error strategies</mark> | Intermediate artifacts |
| <mark>Condensed summaries from specialists</mark> | Raw tool output |

### The summarization principle

When a specialist returns results, <mark>the orchestrator should receive a **structured summary**</mark>, not the full context. This is where subagents shineâ€”they process information in their isolated context and return only what the orchestrator needs to make its next decision.

**Good specialist output:**
```
## Research summary
- 12 files affected (listed in appendix)
- Current pattern: session-based auth with JWT tokens
- Migration risk: HIGH (3 shared services depend on current auth)
- Recommended approach: adapter pattern for backward compatibility
- Estimated effort: 4-6 hours
```

**Poor specialist output:**
```
I read through all the files and here's what I found. In src/auth/login.ts
there's a function called authenticateUser that takes... [500 lines of
paraphrased code analysis]
```

For a detailed treatment of information flow patterns, data contracts, and token management across orchestrations, see [How to Manage Information Flow During Prompt Orchestrations](./12.00-how_to_manage_information_flow_during_prompt_orchestrations.md) and [How to Optimize Token Consumption During Prompt Orchestrations](./13.00-how_to_optimize_token_consumption_during_prompt_orchestrations.md).

---

# ğŸ’¡ Key design principles

After studying orchestration patterns across many workflows, these principles consistently distinguish effective orchestrators from fragile ones:

## 1. Start simple, <mark>add complexity when observed</mark>

Don't begin with a five-specialist architecture. Start with a single agent. When you observe specific failure modesâ€”context overflow, role confusion, lack of checkpointsâ€”add the minimal orchestration that addresses those failures.

## 2. Define <mark>roles by tools, not just instructions</mark>

Instructions guide behavior. Tool access *constrains* it. A reviewer with `edit` tools will eventually make edits, regardless of instructions saying "don't edit files." Remove the tools you don't want used.

Role specialization extends to model selection. In a <mark>model-per-role</mark> pattern, each agent uses the model best suited to its functionâ€”a reasoning-heavy model for planning, a code-optimized model for implementation, a design-focused model for UI work. Burke Holland demonstrates this approach in his [ultralight orchestration framework](../../01.00-news/20260214.3-burke-holland-orchestrations/summary.md), assigning a different model to each of four agents. Specific model choices are inherently volatileâ€”evaluate capabilities at time of use rather than hardcoding recommendations. See [article 08](./08.00-how_to_optimize_prompts_for_specific_models.md) for model family strengths and weaknesses.

## 3. Design <mark>explicit handoff contracts</mark>

Every phase transition should define:
- **What** information passes (structured format, not free text)
- **How much** information passes (summaries, not raw data)
- **When** the transition happens (explicit conditions, not "when ready")

## 4. Set <mark>iteration limits early</mark>

Unbounded loops are the most common orchestration failure. Set limits in the orchestrator's instructions and enforce them. Three iterations of implementâ†’review is usually sufficient.

## 5. <mark>Prefer sequential over parallel by default</mark>

Parallel execution is faster but harder to debug. Start sequential where each phase's output is visible and inspectable. Switch to parallel only when you've validated that the phases are truly independent.

## 6. <mark>Keep the orchestrator thin</mark>

The orchestrator should coordinate, not process. If your orchestrator instructions are longer than your specialist instructions, you're probably doing too much work in the coordinator. Move processing logic to specialists.

## 7. <mark>Plan for failure at every transition</mark>

Every handoff can fail. Every specialist can produce poor results. Design your orchestrator to detect failures early and have explicit recovery strategiesâ€”don't assume the happy path.

## 8. <mark>Validate before you build</mark>

Use the [Use Case Challenge methodology](#-the-use-case-challenge-methodology) before investing in a multi-agent orchestration. Many complex-seeming tasks are better handled by a single agent with clear instructions.

## 9. <mark>Delegate goals, not solutions</mark>

Models naturally want to micromanage. When an orchestrator delegates to a sub-agent, it tends to include step-by-step instructions for *how* the sub-agent should workâ€”overriding the sub-agent's own expertise. You have to explicitly prevent this.

> "These models think they know everything, and so you have to really go out of your way to make sure that they don't do that."
> â€” Burke Holland, Senior Cloud Advocate, Microsoft

**Address both sides of the delegation boundary:**

- **Orchestrator side** â€” Instruct the orchestrator to describe *what* it needs (goals, constraints, acceptance criteria) without dictating *how* to achieve it. For example: "Delegate tasks to specialists. Describe the desired outcome. Do not include implementation steps."
- **Sub-agent side** â€” Include a counter-instruction that grants autonomy: "Question everything you're told. Make your own decisions." This lets the sub-agent push back against over-specified instructions.

This principle complements "Keep the orchestrator thin" (Principle 6). A thin orchestrator *coordinates*; a non-micromanaging orchestrator also *trusts*. Combined, they produce specialists that leverage their full tool access and model strengths rather than blindly following prescriptive steps.

## Practical pattern: Plan â†’ Generate â†’ Implement

Many of the principles above converge in a concrete three-phase workflow pattern demonstrated by Burke Holland (Senior Cloud Advocate, Microsoft). It's a practical example of model routing, context separation, and thin orchestration working together:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PHASE 1 â€” PLAN (premium model, e.g., Claude Opus 4.5)          â”‚
â”‚  Prompt file researches the codebase and produces a              â”‚
â”‚  branch-oriented plan where each step is a testable commit       â”‚
â”‚                                                                  â”‚
â”‚  Output: plan.md                                                 â”‚
â”‚  âš ï¸ Clear context window after this phase                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  PHASE 2 â€” GENERATE (premium model)                              â”‚
â”‚  Prompt file takes plan.md and writes all implementation         â”‚
â”‚  code into a markdown documentâ€”step by step, with checkboxesâ€”    â”‚
â”‚  without modifying the project                                   â”‚
â”‚                                                                  â”‚
â”‚  Output: implementation.md (~2,000 lines)                        â”‚
â”‚  âš ï¸ Clear context window after this phase                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  PHASE 3 â€” IMPLEMENT (free model, e.g., GPT-4.1 mini)           â”‚
â”‚  Custom agent implements code verbatim from                      â”‚
â”‚  implementation.mdâ€”one step at a time, stopping after each       â”‚
â”‚  so you can test, stage, and commit                              â”‚
â”‚                                                                  â”‚
â”‚  Output: a clean PR built from incremental commits               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Why this pattern works:**

- **Model routing** (Principle 2) â€” Premium models handle reasoning-heavy work (planning, code generation). A free model handles the low-reasoning execution step, saving premium requests.
- **Context separation** â€” Clearing the context window between phases prevents [context rot](12.00-how_to_manage_information_flow_during_prompt_orchestrations.md#context-rot-why-context-management-is-urgent). Each phase starts fresh with only the artifact it needs.
- **Thin orchestration** (Principle 6) â€” The orchestrator is the developer. Phases 1 and 2 are prompt files; Phase 3 is a custom agent. No complex coordinator agent is needed.
- **Explicit handoff contracts** (Principle 3) â€” Each phase produces a structured markdown document that the next phase consumes. The contract is the file itself.

**Key implementation details:**

- **Prompt files** for Phases 1 and 2 use the `model:` frontmatter property to route to premium models, regardless of the active model in chat
- **The custom agent** for Phase 3 is deliberately instructed to implement verbatimâ€”it should not reinterpret or "improve" the generated code
- **Context clearing** is manual: start a new chat session between phases

This pattern exemplifies the "start simple" principle (Principle 1)â€”it doesn't require multi-agent orchestration infrastructure. Three files and manual context clearing can achieve results that would otherwise need a complex coordinator.

For a full orchestration case study using automated multi-agent coordination, see [Orchestrator Design Case Study](./20-how_to_create_a_prompt_interacting_with_agents.md).

---

# ğŸ¯ Conclusion

Orchestrator prompts are the architectural glue that holds multi-agent workflows together. The key takeaways:

- **Orchestrators coordinate, they don't process** â€” Keep the coordinator focused on phase sequencing, delegation, and quality gates
- **Specialize by tool access** â€” The most effective way to enforce role boundaries is through tool restriction, not just behavioral instructions
- **Design phase transitions explicitly** â€” Clear handoff contracts with structured data prevent information loss between phases
- **Validate before building** â€” The Use Case Challenge methodology prevents over-engineering; always start with the simplest approach that works
- **Set limits everywhere** â€” Iteration caps, recursion prevention, and error handling strategies prevent orchestrations from running away

### What's next

Now that you understand orchestrator design patterns, explore the implementation details:

- **[How to Design Subagent Orchestrations](./11.00-how_to_design_subagent_orchestrations.md)** â€” Learn the `runSubagent` tool, context isolation mechanics, the `agents` property, and the coordinator-worker pattern
- **[How to Manage Information Flow](./12.00-how_to_manage_information_flow_during_prompt_orchestrations.md)** â€” Data contracts, communication pathways, and information exchange patterns across the customization stack
- **[How to Optimize Token Consumption](./13.00-how_to_optimize_token_consumption_during_prompt_orchestrations.md)** â€” Token budgeting, context compression, and cost management strategies for multi-agent workflows
- **[Orchestrator Design Case Study](./20-how_to_create_a_prompt_interacting_with_agents.md)** â€” A full implementation walkthrough applying these patterns to a real agent-writing workflow

---

# ğŸ“š References

## Official documentation

**[VS Code: Custom Agents](https://code.visualstudio.com/docs/copilot/customization/custom-agents)** `[ğŸ“˜ Official]`  
Microsoft's official documentation for custom agents in VS Code. Covers agent file structure, frontmatter properties, handoffs, tool configuration, and the `agents` property for subagent restriction. The definitive reference for agent design.

**[VS Code: Agents Overview](https://code.visualstudio.com/docs/copilot/agents/overview)** `[ğŸ“˜ Official]`  
Comprehensive overview of agent types (local, background, cloud, third-party), session management, and hand-off mechanics. Essential for understanding execution contexts and when to use each agent type.

**[VS Code: Subagents](https://code.visualstudio.com/docs/copilot/agents/subagents)** `[ğŸ“˜ Official]`  
Official documentation for subagent execution, context isolation, orchestration patterns (coordinator-worker, multi-perspective review), and the `runSubagent` tool. The foundation for understanding delegation mechanics.

**[VS Code: Copilot Customization Overview](https://code.visualstudio.com/docs/copilot/copilot-customization)** `[ğŸ“˜ Official]`  
High-level overview of all Copilot customization optionsâ€”instructions, prompts, agents, skills, and hooks. Explains how different customization types compose together in an orchestration.

**[VS Code: Chat Sessions](https://code.visualstudio.com/docs/copilot/chat/chat-sessions)** `[ğŸ“˜ Official]`  
Session management including context windows, checkpoints, message queuing, and steering. Helps understand how orchestrator sessions accumulate context and how to manage session lifecycle.

## ğŸ“ Related articles in this series

**[How GitHub Copilot Uses Markdown and Prompt Folders](./02-getting-started/01.00-how_github_copilot_uses_markdown_and_prompt_folders.md)** `[ğŸ“™ Internal]`  
Foundation article explaining Copilot's file-based customization system. Establishes the mental model for how instructions, prompts, agents, and skills compose together.

**[How to Structure Content for Copilot Agent Files](./04.00-how_to_structure_content_for_copilot_agent_files.md)** `[ğŸ“™ Internal]`  
Detailed guide for individual agent file designâ€”frontmatter properties, tool selection, instruction writing. A prerequisite for designing specialist agents in an orchestration.

**[How to Use Agent Hooks for Lifecycle Automation](./09.00-how_to_use_agent_hooks_for_lifecycle_automation.md)** `[ğŸ“™ Internal]`  
Hooks provide deterministic policy enforcement that complements orchestrator coordination. Use hooks for security gates, audit trails, and automated quality checks at lifecycle events.

**[How to Design Subagent Orchestrations](./11.00-how_to_design_subagent_orchestrations.md)** `[ğŸ“™ Internal]`  
Deep dive into the `runSubagent` tool, context isolation mechanics, `user-invokable` and `disable-model-invocation` properties, and practical orchestration patterns. The implementation companion to this conceptual article.

**[How to Manage Information Flow During Prompt Orchestrations](./12.00-how_to_manage_information_flow_during_prompt_orchestrations.md)** `[ğŸ“™ Internal]`  
Theory and practice of information flow across the customization stackâ€”context window dynamics, communication pathways, data contracts, and handoff protocols.

**[How to Optimize Token Consumption During Prompt Orchestrations](./13.00-how_to_optimize_token_consumption_during_prompt_orchestrations.md)** `[ğŸ“™ Internal]`  
Nine strategies for managing token budgets in multi-agent workflowsâ€”context compression, model selection, summarization patterns, and cost monitoring.

**[Orchestrator Design Case Study](./20-how_to_create_a_prompt_interacting_with_agents.md)** `[ğŸ“™ Internal]`  
Full implementation walkthrough applying orchestration patterns from this article to a real agent-writing workflow with four specialists.

<!-- 
---
validations:
  grammar: {last_run: null, model: null, outcome: null, issues_found: 0}
  readability: {last_run: null, model: null, outcome: null, flesch_score: null, grade_level: null}
  understandability: {last_run: null, model: null, outcome: null, target_audience: null}
  structure: {last_run: null, model: null, outcome: null, has_toc: true, has_introduction: true, has_conclusion: true, has_references: true}
  facts: {last_run: null, model: null, outcome: null, claims_checked: 0, sources_verified: 0}
  logic: {last_run: null, model: null, outcome: null, flow_score: null}

article_metadata:
  filename: "10.00-how_to_design_orchestrator_prompts.md"
  created: "2026-02-22"
  last_updated: "2026-02-22"
  version: "1.0"
  status: "draft"
  version_history:
    - date: "2026-02-22"
      changes: "Initial article creationâ€”orchestrator design fundamentals extracted from article 20 and expanded"
  word_count: 3200
  reading_time_minutes: 15
  primary_topic: "Orchestrator Prompt Design for Multi-Agent Workflows"

cross_references:
  related_articles:
    - "01.00-how_github_copilot_uses_markdown_and_prompt_folders.md"
    - "04.00-how_to_structure_content_for_copilot_agent_files.md"
    - "09.00-how_to_use_agent_hooks_for_lifecycle_automation.md"
    - "11.00-how_to_design_subagent_orchestrations.md"
    - "12.00-how_to_manage_information_flow_during_prompt_orchestrations.md"
    - "13.00-how_to_optimize_token_consumption_during_prompt_orchestrations.md"
    - "20-how_to_create_a_prompt_interacting_with_agents.md"
  series: "Prompt Engineering for GitHub Copilot"
  position: 10
  previous: "09.00-how_to_use_agent_hooks_for_lifecycle_automation.md"
  next: "11.00-how_to_design_subagent_orchestrations.md"
  prerequisites:
    - "01.00-how_github_copilot_uses_markdown_and_prompt_folders.md"
    - "04.00-how_to_structure_content_for_copilot_agent_files.md"
---
-->
